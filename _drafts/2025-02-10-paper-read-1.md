---
title: '[论文笔记] DeepSeek-V3的推理优化技术'
date: 2025-02-10
permalink: /posts/2025/02/paper-read-1/
tags:
  - Paper Read
tags:
  - LLM
  - Inference
---

[DeepSeek-V3论文原文](https://arxiv.org/abs/2412.19437)

因为我是研究推理优化方向的，所以本文主要从推理角度来看DeepSeek
Deepseek-V3技术报告中的推理优化笔墨并不太多，但是包含了很多新技术，在此简要分析。
原文中直接描述Inference and Deployment的部分只有18-20页，总共一页半，很少。但是还要考虑到其集群部署的方案、KV Cache压缩的方案等技术做一个总结。

## 大规模集群部署




## DP分离推理优化

DP分离推理优化（Prefill和Decode阶段分离）也是24年前沿研究方向，结合[LLM Inference Roofline](https://arxiv.org/abs/2402.16363)分析，大模型因为其庞大的参数量，会导致Decode阶段硬件性能发挥不足。
具体而言，Prefill阶段因为要并行计算大量token之间的Attention，所以受限于GPU计算能力；而Decode阶段只需要计算最新token和之前token的相关性，计算量大大降低，但是大模型巨量参数仍然要从HBM载入计算核心，导致载入的IO时间反而成为性能瓶颈。
总的来说，Prefill阶段是compute bound的，Decode阶段是memory bound的，这在相关论文中已经讨论很多，不再赘述。
Deepseek-V3论文应该是我见到的第一篇MoE模型分布式推理中使用DP分离技术的。之前华为已经先一步探索了多模态大模型的DP分离技术，但MoE模型的DP分离技术应该Deepseek是第一个发布的。
MoE模型相较于文本大模型，使用了多专家模型推理，每轮前向推理只会激活部分专家。这使得DP分离调度多了“不同专家模型”这一个新维度。

但是Deepseek-V3这篇论文的DP分离非常工程化解决，
**补充Deepseek的具体方案**

虽然论文中提到已经在开发动态DP分离方案，但是如果路由模型在训练过程中没有针对性的策略，还是要面对路由不可控的问题，我估计MoE模型的DP分离调度在2025年会是校企合作的一个热点。





## KV Cache优化：Multi-Head Latent, MLA








